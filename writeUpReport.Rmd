---
title: "Math 154 Write Up"
author: "Maddi Cowen, Ciaran Evans, Samantha Morrison"
date: "12/4/2015"
output: html_document
---

##Data Collection

For our project, we are combining meteorological data with recorded storm observations in the North Atlantic.  To gather this data, we must scrape both storm data and meteorological data and then integrate these two datasets.  The storm data came from the NOAA [International Best Track Archive for Climate Stewardship](http://www.ncdc.noaa.gov/ibtracs/index.php?name=ibtracs-data) (IBTrACS) , and the meterological data comes from the NOAA [National Data Buoy Center](http://www.ndbc.noaa.gov/historical_data.shtml), where we are using historical standard meterological data collected by buoys.

To integrate these two dataset, we wanted to take meteorological buoy data that measured different variables of each storm in the storms dataset.  First we noted that we wanted each row of our dataset to represent a portion of the storm with a location in space and time.  So each row was a storm with a specific latitude, longitude, time, and date.  We then would find the buoys within a certain radius of this storm location/time and compute the average meterologial conditions (such as averaging the windspeed measured by several buoys nearby this storm location/time).  

###First Collect Storm Data

However, first we must bring in and clean the data on the storms from NOAA IBTrACS.  NOAA [International Best Track Archive for Climate Stewardship](http://www.ncdc.noaa.gov/ibtracs/index.php?name=ibtracs-data) has datasets on the storms that are sorted by the year the storm was in.  First we downloaded all the csv files in the years we wanted (2000 to 2015).  An example of how the storm data by year that IBTrACS has as csv files is Figure 1.   


![ Figure 1. IBTrACS Storm Data By Year](stormByYearIBTrACS.PNG)  

Figure 1. IBTrACS Storm Data By Year  



 We saved these files as storm*year* on our computer.  Each dataset looks approximately like Figure 2.
![ Figure 2. IBTrACS Storm Data Original File](stormCSV2015.PNG)  

Figure 2. IBTrACS Storm Data Original File  


After saving the datasets as storm*year*, we removed the first row (IBTrACS -- Version: v03r07 ) and the third row that gives units for the variable, so when we read in the csv files the header would appear correctly.

To read in the csv files for the storm data, we used a for loop that read in each storm data set and put it in a list of datasets called allStorms.  Then for each dataset storm*year*, a for loop was used to discard all columns after 19 since these were not variables of interest and often had lots of missing data (labelled as -999).  

Finally, a for loop was used to removed all rows in each storm*year* dataset that has missing data (-999) for latitude and longitude.  These cleaner datasets were assigned to the list cleanStorms.  We wanted to combine this list of all the storm datasets into one large dataset of all the storms from 2000 to 2015.

So first, we checked to make sure the order of all the variables were the same between datasets.  Then combined them into one large dataset called finalStorms.  We further cleaned finalStorms by changing the ISO\_time variable to ymd\_hms using lubridate.  Then we selected out the information we wanted by keeping the variables Basin, Serial\_Num, Latitude, Longitude, time.  We filtered by Basin NA to get storms in the North Atlantic basin, which is where the buoys (the meterological data) are located.



```{r, message=FALSE}
# all the packages needed for the data scraping and cleaning
require(dplyr)
require(tidyr)
require(geosphere) # to calc distances between latitude/longitude of points
require(lubridate) # standardize dates and times
require(XML) # scrape buoy lat/long data from web

```


```{r, eval= FALSE}

# data from http://www.ncdc.noaa.gov/ibtracs/index.php?name=ibtracs-data
# ftp://eclipse.ncdc.noaa.gov/pub/ibtracs/v03r07/all/csv/year/

# save portion of name of storm files that differed between files
stormYearsFile <- c("2000.csv","2001.csv","2002.csv","2003.csv","2004.csv","2005.csv","2006.csv","2007.csv","2008.csv","2009.csv","2010.csv","2011.csv","2012.csv","2013.csv","2014.csv", "2015.csv")

# create a list to store these storm datasets in separately
allStorms <- vector("list",length(stormYearsFile)) 

# pull in csv files and assign each dataset to a different section of the list
for(i in 1:length(stormYearsFile)){
  allStorms[[i]] <-read.csv(paste("~/Comp-Stats-Project/storm",stormYearsFile[i], sep=""))
}

# only keep the first 19 variables (columns)
for(i in 1:length(allStorms)){
  allStorms[[i]] <- allStorms[[i]][,1:19]
}

# store cleaner data in new list of datasets
cleanStorms <- vector("list",length(stormYearsFile))

# remove all rows that have -999 (missing data) as latitude or longitude
for(i in 1:length(allStorms)){
  cleanStorms[[i]] <- allStorms[[i]]%>% 
    filter(Latitude!=-999) %>% 
    filter(Longitude!=-999)
}

# check all storms datasets have same order of variables
checkingStorms <- c()
for(i in 2:length(cleanStorms)){
  checkingStorms[i-1] <- sum(names(cleanStorms[[1]]) == 
                             names(cleanStorms[[i]]))
}


# combine all storm datasets into 1 big dataset
finalStorms <- cleanStorms[[1]]
for(i in 2: length(cleanStorms)){
  finalStorms <- rbind(finalStorms, cleanStorms[[i]])
}

#add new column of time in ymd_hms type setup using parse date time fuction
finalStorms <- finalStorms %>% mutate(time = parse_date_time(ISO_time, "%m%d%Y %H%M")) %>%
  filter(year(time) != 1999) # dont want storms in 2000 season that start in 1999

# only save variables we want and choose storms in Basin NA (north atlantic)
stormsNorthAtl <- finalStorms %>% 
  dplyr::select(Basin, Serial_Num, Latitude, Longitude, time, Wind.WMO., Wind.WMO..Percentile) %>%
  mutate(BasinChar = as.character(Basin))%>% filter(BasinChar == " NA") 

names(stormsNorthAtl)[6] <- "Wind_WMO"  # in knots
names(stormsNorthAtl)[7] <- "Wind_WMO_Perc" # in percentiles

# save dataset for use later
dput(stormsNorthAtl, "stormsNorthAtl.txt")

```

```{r}

stormsNorthAtl <- dget("stormsNorthAtl.txt")
head(stormsNorthAtl,3)

```

###Getting Buoy Locations (combining Storm and Buoy Data)

When we integrate the storm and buoy data, ideally we want each row to be a storm at a specific time and location (latitude/longitude) with meteorological conditions averaged over all the buoys near the storm location/time.  For instance we want our data to look like this:

![](exTable.PNG)

Figure 3.  Example of what we want our data to look at.  Note: data in this example is not accurate.  

However, to do this, we need to get the meteorological data in buoys for the area around the storm time/location.  To do this, we will match the year of the storm row to the NOAA buoy records and determine all the buoys that have historical meterological data at this year.  Then we will determine which buoys that have data are closest to the storm location (latitude and longitude).  We will do futher refinement to only obtain data for buoys close enough to the storm location and around the same time. 

Our very first step will be to determine which NOAA buoys are in the correct location and have the information we need, and then make a list of those buoys.

A list of all stations can be found at [http://www.ndbc.noaa.gov/to_station.shtml](http://www.ndbc.noaa.gov/to_station.shtml); this is what part of the page looks like:

![](noaaBuoyList.JPG)

Each link is a different buoy number, and clicking on it will take you to the page for that specific buoy.  The page also contains lots of non-buoys stations, such as oil rigs, which we will have to deal with later. Let's look at one of the buoys, say [41046](http://www.ndbc.noaa.gov/station_page.php?station=41046). If we scroll down to the bottom of the page, we get a link to historical data:

![](buoy41046Link.JPG)

Clicking on this link takes us to a page ( [http://www.ndbc.noaa.gov/station_history.php?station=41046](http://www.ndbc.noaa.gov/station_history.php?station=41046) ) with historical data, including standard meteorological data:

![](buoy41046StandardMeteorological.JPG)

So buoy 41046 has historical meteorological data for 2007 - 2014, which is great for us!  Here's what the 2007 data ( [http://www.ndbc.noaa.gov/view_text_file.php?filename=41046h2007.txt.gz&dir=data/historical/stdmet/](http://www.ndbc.noaa.gov/view_text_file.php?filename=41046h2007.txt.gz&dir=data/historical/stdmet/) ) looks like:

![](buoy41046_2007data.JPG)


But to get it, we would have to click on each link for each year, which takes us to a text file.  Since there are many buoys and many different years, getting all the data manually would be tedious and unproductive.  Instead, we will write some code to scrape it from the NOAA website.  First, we take all of the buoy numbers from [http://www.ndbc.noaa.gov/to_station.shtml](http://www.ndbc.noaa.gov/to_station.shtml). The source for the webpage is in HTML; each hyperlink has a specific tag, and since each buoy number is a hyperlink on the page we will grab all of the hyperlinks.  The package XML allows us to do this; the htmlTreeParse() function converts the HTML content of the webpage to a useful structure, and then xpathApply() is used to select all the hyperlinks by selecting for the hyperlink tags.

```{r, eval = FALSE}

# We'll use the XML package to get info from web pages

# Read and parse HTML file
buoylist.html = htmlTreeParse('http://www.ndbc.noaa.gov/to_station.shtml',
                         useInternal = TRUE)
# make a vector of each hyperlink
buoylist.text = unlist(xpathApply(buoylist.html, "//a/@href"))

```

We now have a vector containing each hyperlink.  To get all the ones corresponding to buoys, we use the grep() function from the base package to find matches to the string pattern "station=" in the vector. Having found the indices of strings that contain the pattern "station=", we pull out the station number from each of those strings by selecting the substring of length 5 that start immediately after the "=" sign:

```{r, eval = FALSE}

# indices of hyperlinks that correspond to NOAA observation stations
hrefIndices <- grep("station=", buoylist.text)

# create vector to hold the buoy numbers
buoyNums <- c()

for(i in 1:length(hrefIndices)){
  href <- hrefIndices[i]  # look at each buoy index in turn
  refString <- buoylist.text[href]  # look at hyperlink at that index
  # substring starts after '=' sign
  startind <- unlist(gregexpr('=',refString)) + 1  
  # take substring of length 5, starting right after '=' sign
  buoyNums[i] <- substr(refString, startind, startind + 4)
}

```


Why do we only look at strings of lengths five? It turns out that's how long a buoy ID number is, according to the NOAA documentation at [http://www.ndbc.noaa.gov/staid.shtml](http://www.ndbc.noaa.gov/staid.shtml) :

![](buoyIDInfo.JPG)

And in fact, as we are only interested in buoys in the North Atlantic, this documentation tells us that we need only consider those buoy numbers beginning with 41, 42, or 44, so let's select just those values from the vector of buoy numbers we just created:

```{r, eval = FALSE}

buoyNums <- buoyNums[substr(buoyNums, 1,2) %in% c("41", "42", "44")]

```

Now that we have a list of buoys, we want to know the location of each buoy.  Each buoy has its latitude and longitude on the page reached by following its hyperlink.  For example, if we click on buoy [42042](http://www.ndbc.noaa.gov/station_page.php?station=42042), then we can see that its latitude and longitude are 29.880 N, 88.320 W :

![](buoy42042PicAndLoc.JPG)

Since we don't want to have to manually collect these latitude and longitude values, we will again scrape them from the HTML source for each buoy webpage.  The address for buoy 42042 is [http://www.ndbc.noaa.gov/station_page.php?station=42042](http://www.ndbc.noaa.gov/station_page.php?station=42042), and in fact every buoy has the same address up until the final 5 characters, which hold the buoy ID number. Since we have a vector containing each buoy number, we can loop through that vector and paste each number in the vector into the basic address form. Then, we can find the latitude and longitude on the HTML for that page.

```{r, eval = FALSE}

lat <- c() # vector to hold buoy latitudes
lon <- c() # vector to hold buoy longitudes

# loop over all the buoy numbers we collected
for(i in 1:length(buoyNums)){
  
  # get the HTML source for each buoy page
  doc.html = 
    htmlTreeParse(paste('http://www.ndbc.noaa.gov/station_page.php?station=',
                        buoyNums[i], sep=""),
                           useInternal = TRUE)
  # get all the paragraph (<p>) elements from the HTML
  doc.text = unlist(xpathApply(doc.html, '//p', xmlValue))
  
  # search for ' N ', which will appear immediately after the latitude
  startind1 <- min(unlist(gregexpr(' N ',doc.text[1]))) - 6 # latitude start
  stopind1 <- startind1 + 5 # latitude end
  startind2 <- stopind1 + 4 # longitude start
  stopind2 <- startind2 + 5 # longitude end
  
  # store the latitude and longitude in the vectors we made
  lat[i] <- as.numeric(substr(doc.text[1], startind1, stopind1))
  lon[i] <- as.numeric(substr(doc.text[1], startind2, stopind2))
}

# remove those which are missing latitude and longitude
buoyNums <- buoyNums[-which(is.na(lat))]
lat <- lat[-which(is.na(lat))]
lon <- lon[-which(is.na(lon))]

```


Note: When you run the above chunk you get NA introduced by coercion, which is removed when we removed the buoys that are missing latitude and longitude.
In the last few lines, we've removed those buoys without latitude and longitude information.  Since we need the buoy locations to match them up with storms data, we can't use a buoy if we don't have its location.

We are interested in the years 2000 - 2015.  Some of the buoys we selected have data for some of these years, while some do not have data for any of these years (for example, old buoys from the '70s and '80s). So, we need to go through each buoy and determine which years it has data for.  The result will be a matrix which contains the number of each buoy, its location, and a boolean variable for each year from 2000 to 2015 which is TRUE if it has data for that year and false otherwise. 

To get the data, we note that the historical data for each buoy can be found at an address that differs only in the buoy number (again, this is the last 5 characters).  For example, for buoy 42042 the address is [http://www.ndbc.noaa.gov/station_history.php?station=42042](http://www.ndbc.noaa.gov/station_history.php?station=42042). Here's what that webpage looks like:

![](buoy42042HistoricalDataPage.JPG)

We want to know for which years the buoys have standard meteorological data.  For buoy 42042, as we can see in the image, data only exists for year 2000. We'll use our HTML scraping tools again to find the "Historical Data" list in the HTML source. Then, since standard meteorological data is the first item in the historical data list, we select all the numbers in that item.

```{r, eval= FALSE}

# interested in years 2000 - 2015
stormYears <- seq(from=2000, to=2015, by=1)

# make a matrix to hold buoy number, latitude, longitude, and 
#  indicator variables for which years it has data
buoyLocsAndYears <- as.data.frame(matrix(nrow=length(buoyNums), ncol=19))
names(buoyLocsAndYears) <- c("BuoyNumber", "Latitude", "Longitude", 
                             paste("year", as.character(stormYears), sep=""))

# loop over all the buoys
for(i in 1:length(buoyNums)){
  # get HTML source for the station history for each buoy in turn
  historical.html <- 
    htmlTreeParse(paste('http://www.ndbc.noaa.gov/station_history.php?station=',
                        buoyNums[i]),
                                   useInternal = TRUE)
  
  # find all the list (<li>) elements in the HTML source
  historical.text = unlist(xpathApply(historical.html, '//li', xmlValue))
  
  # find the list element that is 'Historical data'
  histString <- historical.text[grep('Historical data', historical.text)]
  
  # take the numbers from the first entry of the 'Historical data' list
  # these are the numbers that lie between the first and second '\n' substrings
  endpoints <- unlist(gregexpr('\n', histString))[c(1,2)]
  yearString <- substr(histString, endpoints[1]+2, endpoints[2]-1)
  
  # split the string (by empty spaces) to make each year a separate 
  #  element of a numeric vector
  yearsOfHistData <- as.numeric(unlist(strsplit(yearString, " ")))
  
  # for each year 2000 - 2015, test if it is in the set of years for that buoy
  #  save the results in the matrix row corresponding to that buoy
  buoyLocsAndYears[i, 4:19] <- stormYears %in% yearsOfHistData
}

```


We now have a matrix that contains the years of data for each buoy. We also want the number and location, so let's add those.  So that our latitude and longitude will be simply numeric, without the need for degrees N or W, we will use the format (found also in the IBTrACS storms data) of assigning a sign to latitude and longitude instead of a compass direction. Latitudes north of the equator and longitudes east of the prime meridian are positive, while latitudes south of the equator and longitudes west of the prime meridian are negative.  So, for example, 35.56 N 54.78 W becomes the pair (35.56, -54.78). This change is helpful both to match with the IBTrACS data and to avoid the need for character strings in storing our location data.

```{r, eval = FALSE}

# store buoy number in matrix
buoyLocsAndYears$BuoyNumber <- buoyNums 

# store latitude in matrix (all latitudes are N of equator)
buoyLocsAndYears$Latitude <- lat 

# store longitude in matrix (all longitudes are W of prime meridian)
buoyLocsAndYears$Longitude <- -1*lon

# save table for use later
dput(buoyLocsAndYears, "buoyLocsAndYears.txt")

```

```{r}

buoyLocsAndYears <- dget("buoyLocsAndYears.txt")
head(buoyLocsAndYears,3)

```


However, there is a problem with the buoy collection we've just performed: some of the buoys do not have the data we need! There are two ways this can happen, either the buoy simply does not have data for the years 2000 - 2015 (its an old buoy) or it is not really a buoy.  "But how can it not be a buoy?" you ask, "since we selected the buoy numbers following the NOAA ID numbering system?" Well, as we briefly hinted at above, NOAA has information on a whole variety of stations (e.g., oil rigs), not just buoys.  The numbering system applies to ALL stations, and so we inadvertently selected a bunch of other types of station as well.  Like station number 42386, which belongs to Chevron:

![](ChevronStations.JPG)

We need to remove the buoys that do not have the data we want.  The way we will do this is to actually try and get the data, and see if we can.  It turns out that each year of standard meteorological data is found in a text file at a web address with the same format, differing only by buoy ID number and the year.  For example, data from 2014 for buoy 42036 is found at [http://www.ndbc.noaa.gov/view_text_file.php?filename=42036h2014.txt.gz&dir=data/historical/stdmet/](http://www.ndbc.noaa.gov/view_text_file.php?filename=42036h2014.txt.gz&dir=data/historical/stdmet/). For each buoy, we will check each year to see whether it has standard meteorlogical data for that year.  If it does for any of the years, we will store the buoy number.  We use the try() function to attempt to get data from each of these web addresses, and we make 10 attempts to get the data.  If we fail after 10 attempts, we move on to the next year.

```{r, eval= FALSE}

# final dataframe of buoy locations and years of data 
# will be called buoyLocationsFixed

buoyLocationsFixed <- buoyLocsAndYears # dataframe that needs cleaning

# numeric vector to hold the buoy numbers we want to keep
isABuoy <- c()

# loop overal all buoys
for(k in 1:(nrow(buoyLocationsFixed))){
  
  # this is TRUE until we find data for the buoy in one of the years 2000 - 2015
  notFoundDataYet <- TRUE
  
  # loop over each year 2000 - 2015
  for(j in 4:(ncol(buoyLocationsFixed))){
    buoyYear <- 1996 + j
    
    # if the historical data exists, it will be at this site
    site <- paste("http://www.ndbc.noaa.gov/view_text_file.php?filename=",
                  buoyLocationsFixed$BuoyNumber[k],
                  "h",buoyYear, ".txt.gz&dir=data/historical/stdmet/", sep ="")
        
    if(notFoundDataYet){  
      # call in buoy data with try command in while loop
      i <- 1 # 1 = failed to connect, 2 = managed to connect
          
      # count the number of times we try and fail to get the data
      countError <- 0
          
      # while we haven't connected, and have tried fewer than 10 times,
      #  try to connect
      while (i < 2 & countError < 10){
        data <- try(read.table(site,sep="", fill=TRUE, header=TRUE,
                               comment.char=""), silent=TRUE)
        
        # if we get a try-error, we failed.  Increase the number of failures
        #  and go back to the beginning of the while loop
        if (class(data) == "try-error") {
          countError = countError + 1
          next
        } else {  # otherwise, we managed to connect, so data exists
          i <- i + 1  # increase i to stop entering the while loop
        }
      }
      
      # if we got a try error all 10 attempts, the class of our 'data'
      #   while be 'try-error', in which case we do nothing
      # otherwise, we got data and we store the buoy number as a good one
      # then set notFoundDataYet to FALSE so that when we look at the next 
      #   year we don't have to try to find data
      #   (if the buoy has data for one year, it doesn't matter 
      #    about the others)
      if(class(data) != "try-error"){
        isABuoy <- c(isABuoy, buoyLocationsFixed$BuoyNumber[k])
        notFoundDataYet <- FALSE
      }
        
    }
  }
  
}

```

Now we'll store the list of buoy numbers that we want to keep, then use the filter() function to select the rows of the buoy locations and years matrix for which the buoy number is in the list of those that we want to keep. Finally, we store the fixed version (buoyLocationsFixed) of the buoy matrix, which contains the ID number, latitude, longitude, and year indicator variables for only those buoys that have the data we want.

```{r, eval = FALSE}
# buoy locations (lat/long) filtered by those that are buoys (i.e. not oil rigs) and that have data
buoyLocationsFixed <- buoyLocationsFixed %>% filter(BuoyNumber %in% isABuoy)

# save the data for use later
dput(buoyLocationsFixed, "buoyLocationsFixed.txt")

```


```{r}

buoyLocationsFixed <-dget("buoyLocationsFixed.txt")

head(buoyLocationsFixed,3)

```


###Combining Storm and Buoy Data (calculating weighted means for buoy data)

Now that we have a dataset *buoyLocationsFixed*  of all the buoys with their locations (latitude/longitude) and that tells us which year each buoy has data for, we can now combine this with the storm dataset *stormsNorthAtl*.

To integrate the storms and buoy data, we want take each row of the storms dataset (which is a storm at a time and location) and compute the average meterological conditions around the storm time/location as measured by buoys.

ADD MORE DESCRIPTIONS LATER

```{r, eval = FALSE}

# time how long this process takes
Sys.time()

# table of buoy num, lat, long, and TRUE/FALSE if have data in years 2000 to 2015
buoyLocations <- dget("buoyLocationsFixed.txt") 

# vector of weighted means of weather variables
weightedMeans <- c()

# loops over every row in stormsNorthAtl dataset
for(k in 1:nrow(stormsNorthAtl)){
  
  # indicates whether this storm row has any buoys near it with data around the time of the storm row
  stormRowHasBuoyData = FALSE
  
  # the storm row we are finding avg weather variables for
  theRow <- stormsNorthAtl[k,]
  
  #column num that corresponds to variable yearX in buoyLocations file
  #yearX is the year of the storm for this row
  yearColNum <- which(names(buoyLocations)==paste("year", year(theRow$time), sep=""))
  
  # only take buoys w data from yearX
  buoyLocsThatYear <- buoyLocations %>% filter(buoyLocations[,yearColNum] == TRUE)
  
  # longitude and latitude of this storm row
  stormCoords <- c(theRow$Longitude, theRow$Latitude)
  
  # function to compute dist from storm coord to buoy (buoy parameter takes long/lat of buoy)
  computeDistanceTo <- function(buoy){
    return(distGeo(stormCoords,buoy))
  }
  
 
  # computes distances of buoys with data in yearX to the storm
  # filter so we only have buoys within 500 km of storm 
  buoyDists <- buoyLocsThatYear %>% mutate(distToStorm = 
             apply(cbind(Longitude, Latitude), 1, computeDistanceTo)) %>%
    filter(distToStorm <= 500000) %>% dplyr::select(BuoyNumber, distToStorm)
  
  # save this buoy numbers so we can get their datasets
  buoyNumber <- buoyDists$BuoyNumber
  buoyYear <- year(theRow$time) # yearX that storm was in
  
  # if there are buoys near the storm, pull in their data
  # otherwise do nothing and go back through for loop over
  # rows of storm data
  if(length(buoyNumber)!= 0){
    
    # dataset where we store all the buoy data for buoys near the storm
    matchingBuoys <- as.data.frame(matrix(nrow = 1,ncol =19))
    names(matchingBuoys) = c("YYYY", "MM", "DD", "hh", "WD", "WSPD", "GST", 
                             "WVHT", "DPD", "APD", "MWD", "BAR", "ATMP" ,
                             "WTMP","DEWP" ,"VIS", "TIDE", "ymdday", "BuoyNumber")
    
    #loops over all buoys near this storm row and pulls in the buoy datasets
    for(j in 1:length(buoyNumber)){
      # url for .txt with buoy data for a certain year
      site <- paste("http://www.ndbc.noaa.gov/view_text_file.php?filename=", 
                    buoyNumber[j], "h", buoyYear, ".txt.gz&dir=data/historical/stdmet/", sep ="")
    
      # call in buoy data with try command in while loop
      
      # count the number of times we try and fail to get the data (due to 404 Errors etc)
      countError <- 0
      i <- 1
      while (i < 2 & countError < 10){ # keep trying to get data unless get 10 error messages, then give up
        data <- try(read.table(site,sep="", fill=TRUE, header=TRUE, comment.char=""), silent=TRUE)
        if (class(data) == "try-error") { # e.g. try error due to 404 Error
          countError = countError + 1
          next # goes back to while loop
        } else {
          i <- i + 1 # got data, exit while loop
        }
      }
    
      # we try to get the data 10 times; if we fail every time,
      # the data will be of class try-error.  
      # if the data is actual buoy data, go on to clean up and average the buoy data
      # if we could not get data (is try error), go onto next buoy Number in loop 
      # and try to pull in that data
      if(class(data) != "try-error"){
        # standardize the buoy datasets variable names 
        
        # clean up if buoy data .txt has comment sign (#) -- 2007 and later
        if(length(grep("X.",names(data)))>0){
          # ie names variable X.YY to YY
          names(data)[grep("X.",names(data))] <-
            substr(names(data)[grep("X.",names(data))],
                   start=3,nchar(names(data)[grep("X.",names(data))]))
          # removes row of variable units
          data <- data[-1,]
          
          # change YY to YYYY, WDIR to WD, PRES to BAR
          names(data)[grep("YY", names(data))] <- "YYYY"
          names(data)[grep("WDIR", names(data))] <- "WD"
          names(data)[grep("PRES", names(data))] <- "BAR"
          
          # remove mm (min col)
          data <- data[,-grep("mm", names(data))]
        } else if(buoyYear>2004) { # only need to remove min to standardize
          # remove mm (min col)
          data <- data[,-grep("mm", names(data))]
        }
          
        # make everything numeric
        data <- as.data.frame(apply(data[,1:(length(data))], 2, as.numeric))
          
        # make new variable ymd_hms w lubridate (set all min 00 and sec to 00)
        nextRow<-data%>%mutate(ymdday=ymd_hms(paste(YYYY,"-", MM,"-",DD, 
                                                    "-", hh, "-", 00, "-", 00)))%>%
          filter(ymdday == theRow$time) #only want data at time matching storm
        
        # if row(s) of buoy data at storm time is not empty, add buoy number onto row  
        if(nrow(nextRow) > 0){
          # indicates this storm row has buoy data at this location and time
          stormRowHasBuoyData = TRUE 
          nextRow <- as.data.frame(c(nextRow, buoyNumber[j]))
          names(nextRow)[ncol(nextRow)] <- "BuoyNumber"
          
          # matchingBuoys holds data for all the buoys near the storm
          # each row of matchingBuoys is a buoy at a certain time
         
          if(j==1){
            # assign first row of buoy data to empty matchingBuoys (to get formatting correct)
            matchingBuoys <- nextRow
          } else {
            # rbind on other nextRows (rows of buoy data) sequentially
            matchingBuoys <- rbind(matchingBuoys, nextRow)
          }
            
        }
      
      }
   
    }
    
    # if this storm row has buoys near it with data around the time of the storm
    # calculate weighted means of this buoy data
    if(stormRowHasBuoyData){
      # join buoy data for that storm row to distance from buoy to storm
      matchingBuoys <- matchingBuoys %>% 
        mutate(BuoyNumber = as.character(BuoyNumber))%>%
        left_join(buoyDists, by = "BuoyNumber")
      
      # converting 999 and 99 and 9999 (missing data) to NA
      matchingBuoys[,c(6:10, 16:17)][matchingBuoys[,c(6:10, 16:17)] == 99] <- NA
      
      matchingBuoys[,c(5, 11, 13:15)][matchingBuoys[,c(5, 11, 13:15)] == 999] <- NA
      
      matchingBuoys[,12][matchingBuoys[,12] == 9999] <- NA
      
      # average rows with same buoy number, with na.rm = TRUE 
      # assume buoy data at close to same time for the same data is repetitive, 
      # so average by buoy Number to prevent overweighting
      matchingBuoys <- matchingBuoys %>% group_by(BuoyNumber) %>%
        summarize_each(funs(mean(., na.rm=TRUE)))
      
      # change to data frame to get formatting correct
      matchingBuoys <- as.data.frame(matchingBuoys)
      
      # function to calculate weighted averages (weighted inversely by distance of buoy to storm row)
      weightedByDistance <- function(x){
        return(weighted.mean(x, w= 1/(matchingBuoys$distToStorm), na.rm = TRUE))
      }
      
      # calculates weighted avgs of buoy variables for each storm row
      meansForRow <- apply(matchingBuoys[,c(6:18)], 2, weightedByDistance)
      
      # if weighted avgs not all NaN then add on corresponding storm rowNum
      if(sum(meansForRow, na.rm=TRUE) > 0){
        meansForRow <- c(meansForRow, k)
        names(meansForRow)[length(meansForRow)] <- "rowNum"
        meansForRow <- as.data.frame(t(as.matrix(meansForRow))) # reformatting
        
        # weightedMeans is a dataframe that has weather variables that are weighted avgs of weather variables
        # from buoys near the storm row for all the storm rows
        if(length(weightedMeans) == 0){ 
          weightedMeans <- meansForRow # assign first row of weighted weather variable to empty vector for correct formatting
        } else {
          weightedMeans <- rbind(weightedMeans, meansForRow) # rbind on weighted weather variables for other storm rows 
        }
      }
    }
  }
}

# to determine when loop ended
Sys.time()

# add row number to storms dataset to allow joining
stormsNorthAtl <- stormsNorthAtl %>% mutate(rowNum = as.numeric(rownames(stormsNorthAtl)))

# added weighted means data onto storms data (only keep storm rows that has buoy data )
stormsWithBuoyData <- left_join(weightedMeans, stormsNorthAtl, by="rowNum")

# save for use later
dput(weightedMeans, "weightedMeans.txt")
dput(stormsWithBuoyData, "stormsWithBuoyData.txt")

# Results
# Start time: 2015-12-10 21:03:42 PST
# End time: 2015-12-11 09:39:46 PST
# Warnings: 49 warnings of form "In file(file, "rt") : unable to connect to 'www.ndbc.noaa.gov' on port 80."

```

```{r}

weightedMeans <- dget("weightedMeans.txt")
stormsWithBuoyData <- dget("stormsWithBuoyData.txt")

# end result of dataset of weighted weather variables for each storm row
# (except skips storm rows that do not have buoy data at that time/location)
head(weightedMeans, 4)

# combined datasets
head(stormsWithBuoyData, 4)

```


### Assessing Data (Neural Networks and Trees)



```{r, message = FALSE}

require(neuralnet)
require(dplyr)
require(tree)
require(randomForest)
require(ggplot2)

```

The variables we were looking at 


```{r}

# take a look at the compiled dataset, look for NAs
summary(stormsWithBuoyData)

# keep variables we want to use and that are not predominately NAs
stormsWithBuoyData.nn <- stormsWithBuoyData %>% 
  dplyr::select(WD, WSPD, GST, WVHT, APD, BAR, ATMP, WTMP, Wind_WMO)

# find which rows have missing data of the variables we kept
rowLabs <- seq(1:nrow(stormsWithBuoyData.nn))
rowLabs <- rowLabs[rowSums(is.na(stormsWithBuoyData.nn)) > 0]

# take out all the rows with missing data
stormsWithBuoyData.nn <- stormsWithBuoyData.nn[-rowLabs,]

## normalize the variables to between 0,1

# min of each variables
mins <- apply(stormsWithBuoyData.nn, 2, min)

# max of each variable
maxs <- apply(stormsWithBuoyData.nn, 2, max)

# look at the data
summary(stormsWithBuoyData.nn)

# normalize the data
stormsWithBuoyData.nn<-stormsWithBuoyData.nn %>% scale(center = mins, scale = maxs - mins)

# reformat into data frame
stormsWithBuoyData.nn <- as.data.frame(stormsWithBuoyData.nn)

# set the seed
set.seed(8)

# split in 3/4 training and 1/4 test
trainStormID <- sample(1:nrow(stormsWithBuoyData.nn),
 round(0.75*nrow(stormsWithBuoyData.nn)))

# training data
trainingStorms <- stormsWithBuoyData.nn[trainStormID,]

# testing data 
testingStorms <- stormsWithBuoyData.nn[-trainStormID,]

# try a neural net with 2 nodes for the one hidden layer
net.storms <- neuralnet(Wind_WMO ~ WD + WSPD + GST + WVHT + APD + BAR + ATMP + WTMP,
                        trainingStorms, hidden = 2)


# look at what the neural net looks like 
print(net.storms)
plot(net.storms)

# calculate test error rate
results <- compute(net.storms, testingStorms[, 1:8])$net.result[,1]

# MSE 
MSE <- sum((testingStorms$Wind_WMO - results)^2)/nrow(testingStorms)
MSE

# compare predicted wind to actual 
plot(testingStorms$Wind_WMO, results ,col='blue',main='Predicted vs Real', xlab = "Real Wind WMO", ylab = "Predicted Wind WMO")
abline(0,1,lwd=2)


```






```{r, cache = TRUE}

# set the seed
set.seed(8)

## fit data with random forest to compare to neural net

# run nested for loops to determine what values of mtry and ntree to use
sizeM <- seq(1,(ncol(trainingStorms)-1), by=1)
numTrees<- seq(20,500, by = 20)

error.rf <- c()
mtrys <- c()
ntrees <- c()



for(i in 1:length(sizeM)){
  for(j in 1:length(numTrees)){
    
    storm.rf <- randomForest(Wind_WMO ~., data = trainingStorms, mtry=sizeM[i],     
                               ntree=numTrees[j])
    yhat.rf <- predict(storm.rf, newdata = testingStorms)
    error.rf <- c(error.rf,sum((yhat.rf - testingStorms$Wind_WMO)^2))
    mtrys <- c(mtrys,sizeM[i])
    ntrees <- c(ntrees,numTrees[j])
  }
}

#make dataframe of error rate, mtry and ntree values
output.rf <- data.frame(error=error.rf,mtry=as.factor(mtrys), ntree=ntrees)

# save data for use later
dput(output.rf, "output.txt")

```


```{r}

# plot to determine which parmeters to use
output.rf <- dget("output.txt")
#plot results to decide which values mtry and ntree to use
output.rf %>% ggplot(aes(x=ntree, y=error, color = mtry, lty = mtry)) +
  geom_line() + geom_point()+ 
  xlab("Number of Trees") + ylab("Test Error Rate") +
  ggtitle("Test Error Rate for Different Parameter Values")


# make model with parameters ntree = 250, mtry = 6
set.seed(47)

storm.model.rf <- randomForest(Wind_WMO ~., data = trainingStorms,
                               mtry = 6, ntree = 250, importance = TRUE)

storm.model.rf

# find test error
yhat.model.rf <- predict(storm.model.rf, newdata = testingStorms)
error.model.rf <- sum((yhat.model.rf - testingStorms$Wind_WMO)^2)
error.model.rf


# see which variables are important

varImpPlot(storm.model.rf, main = "Variable Importance")


```

From the random forest model with the parameters of number of predictors (mtry) of 6 and number of trees (ntree) of 250, the test error rate was `r round(error.model.rf, 3)` %.  From the variable importance plot, we can see that WVHT (wave height) is most important for decreasing MSE and increasing node purity.  The next two important variables are WTMP (wave temp) and APD (average wave period).   